# âš¡ MÃ³dulo Spark

> Aprende a procesar Big Data en memoria con Apache Spark y compararlo con Pandas

---

## ğŸ“Š Dataset: E-commerce

UbicaciÃ³n: `data/`

| Archivo | Registros | DescripciÃ³n |
|---------|-----------|-------------|
| orders.csv | 206,000 | Pedidos de clientes |
| order_products__prior.csv | 3.2M | Productos en cada pedido |
| products.csv | 49,000 | CatÃ¡logo de productos |
| aisles.csv | 134 | Pasillos de supermercado |
| departments.csv | 21 | Departamentos de productos |

**Fuente:** Dataset tipo Instacart (e-commerce de supermercado)

---

## ğŸ“š Notebooks

### 1ï¸âƒ£ [Big_Data.ipynb](Big_Data.ipynb)
**Conceptos de Big Data y caso de estudio**

**Contenido:**
- Â¿QuÃ© es Big Data? (Volumen, Variedad, Velocidad)
- DesafÃ­os del anÃ¡lisis de datos masivos
- Contexto del problema: empresa e-commerce
- MetodologÃ­a de anÃ¡lisis con Big Data
- Desarrollo prÃ¡ctico con Pandas (6 visualizaciones)

**DuraciÃ³n:** ~2 horas

---

### 2ï¸âƒ£ [Analisis_Pandas.ipynb](Analisis_Pandas.ipynb)
**AnÃ¡lisis exploratorio con Pandas (enfoque tradicional)**

**AnÃ¡lisis realizados:**

ğŸ“Œ **1. Carga y preprocesamiento**
- Importar CSVs con Pandas
- Limpieza de datos (valores nulos, duplicados)
- Merge de mÃºltiples tablas

ğŸ“Œ **2. DistribuciÃ³n de pedidos**
- Pedidos por usuario
- Productos mÃ¡s comprados
- Histogramas y distribuciones

ğŸ“Œ **3. Patrones temporales**
- Pedidos por dÃ­a de la semana
- Pedidos por hora del dÃ­a
- VisualizaciÃ³n con grÃ¡ficos de barras

ğŸ“Œ **4. Top productos**
- 5 productos mÃ¡s vendidos
- AnÃ¡lisis por departamento
- Productos por pasillo

ğŸ“Œ **5. Market Basket Analysis**
- Productos comprados juntos
- Grafo de asociaciones con NetworkX
- VisualizaciÃ³n de patrones

**TÃ©cnicas:** Pandas, Matplotlib, Seaborn, NetworkX

**DuraciÃ³n:** ~4 horas

---

### 3ï¸âƒ£ [Analisis_Spark.ipynb](Analisis_Spark.ipynb)
**Mismo anÃ¡lisis con PySpark (enfoque distribuido)**

**Conceptos Spark:**
- **SparkSession:** Punto de entrada a Spark
- **DataFrame:** Estructura de datos distribuida
- **RDD:** Resilient Distributed Dataset
- **Transformations:** Lazy operations (map, filter, select)
- **Actions:** Ejecutan el DAG (count, show, collect)
- **DAG:** Directed Acyclic Graph (plan de ejecuciÃ³n)

**AnÃ¡lisis (igual que Pandas):**
- Carga de CSVs con Spark
- Transformaciones y agregaciones distribuidas
- Joins optimizados
- Window Functions (tÃ©cnica avanzada)
- UDFs (User Defined Functions)

**Optimizaciones:**
- `cache()`: Persistir DataFrames en memoria
- `broadcast()`: Replicar tablas pequeÃ±as
- `repartition()`: Balancear particiones
- `persist()`: Control de almacenamiento

**ComparaciÃ³n de rendimiento:**
- Pandas: Mejor para datasets pequeÃ±os (<1GB)
- Spark: Mejor para datasets grandes (>5GB)

**DuraciÃ³n:** ~5 horas

---

## ğŸ¯ Objetivos de Aprendizaje

âœ… **Pandas:**
- ManipulaciÃ³n de DataFrames
- AnÃ¡lisis exploratorio de datos (EDA)
- VisualizaciÃ³n con Matplotlib
- Limitaciones con datos grandes

âœ… **PySpark:**
- Arquitectura distribuida de Spark
- Transformations vs Actions
- OptimizaciÃ³n de queries
- CuÃ¡ndo usar Spark vs Pandas

âœ… **ComparaciÃ³n:**
- Velocidad de procesamiento
- Uso de memoria
- Escalabilidad
- Casos de uso ideales

---

## ğŸš€ Inicio RÃ¡pido

### Instalar dependencias

```bash
pip install pandas matplotlib seaborn networkx pyspark jupyter
```

### Ejecutar notebooks

```bash
# Iniciar Jupyter
jupyter notebook

# Abrir en orden:
# 1. Big_Data.ipynb
# 2. Analisis_Pandas.ipynb
# 3. Analisis_Spark.ipynb
```

---

## ğŸ“ˆ AnÃ¡lisis Realizados

| AnÃ¡lisis | Pandas | PySpark |
|----------|--------|---------|
| ğŸ“Š DistribuciÃ³n de pedidos | âœ… | âœ… |
| ğŸ“… Patrones temporales | âœ… | âœ… |
| ğŸ† Top 5 productos | âœ… | âœ… |
| ğŸ—‚ï¸ Productos por departamento | âœ… | âœ… |
| ğŸ”— Market Basket Analysis | âœ… | âœ… |
| ğŸªŸ Window Functions | âŒ | âœ… |
| âš¡ Optimizaciones distribuidas | âŒ | âœ… |

---

## ğŸ’¡ Tips

### Para Pandas:
- Usar `dtypes` para verificar tipos de datos
- `info()` muestra memoria usada
- `describe()` para estadÃ­sticas rÃ¡pidas
- `head()` para inspeccionar datos

### Para PySpark:
- Siempre crear `SparkSession` al inicio
- Usar `cache()` en DataFrames reutilizados
- `explain()` muestra el plan de ejecuciÃ³n
- `show()` limita filas por defecto (20)
- Detener sesiÃ³n: `spark.stop()`

---

## ğŸ” Conceptos Clave

**Lazy Evaluation:** Spark no ejecuta hasta llamar una acciÃ³n

**DAG:** Plan de ejecuciÃ³n optimizado automÃ¡ticamente

**Particiones:** Datos divididos para procesamiento paralelo

**Broadcast:** Copia tablas pequeÃ±as a todos los workers

**Window Functions:** Operaciones sobre ventanas de datos (RANK, LEAD, LAG)

---

## ğŸ“¦ Estructura del Dataset

```
data/
â”œâ”€â”€ orders.csv                    # Pedidos (order_id, user_id, ...)
â”œâ”€â”€ order_products__prior.csv     # Productos por pedido
â”œâ”€â”€ products.csv                  # CatÃ¡logo (product_id, name)
â”œâ”€â”€ aisles.csv                    # Pasillos (aisle_id, name)
â””â”€â”€ departments.csv               # Departamentos (dept_id, name)
```

**Relaciones:**
- orders â†’ order_products (order_id)
- order_products â†’ products (product_id)
- products â†’ aisles (aisle_id)
- products â†’ departments (department_id)

---

## ğŸ“ Orden de Estudio Recomendado

1. **Big_Data.ipynb** â†’ Entender el contexto y conceptos
2. **Analisis_Pandas.ipynb** â†’ Aprender anÃ¡lisis tradicional
3. **Analisis_Spark.ipynb** â†’ Comparar con enfoque distribuido

**Tiempo total estimado:** ~11 horas

---

**Â¡Ã‰xito con Spark! âš¡**
