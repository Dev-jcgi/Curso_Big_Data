# ğŸ“š GuÃ­a de Estudio: Big Data con Hadoop y Spark

> **Universidad ISU - Sistemas Inteligentes**  
> **MÃ³dulo:** Big Data y ComputaciÃ³n Distribuida  
> **Ãšltima actualizaciÃ³n:** Febrero 2026

<div align="center">

![Hadoop](https://img.shields.io/badge/Hadoop-3.4.0-yellow?style=for-the-badge&logo=apachehadoop)
![Hive](https://img.shields.io/badge/Hive-3.3.1-orange?style=for-the-badge&logo=apachehive)
![Spark](https://img.shields.io/badge/Spark-3.5-E25A1C?style=for-the-badge&logo=apachespark)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python)

**ğŸ“Š 11 Recursos de Aprendizaje | â±ï¸ ~50 horas | ğŸ¬ 2 Datasets Reales**

</div>

---

## ğŸ“‘ Contenido del Curso

Este workspace contiene material completo para aprender Big Data con **Hadoop** y **Apache Spark**:

### **MÃ³dulo Hadoop** (~35h)
- 8 guÃ­as de instalaciÃ³n, configuraciÃ³n y prÃ¡ctica
- Dataset MovieLens 100K (100,000 registros reales)
- TecnologÃ­as: HDFS, YARN, MapReduce, Hive

### **MÃ³dulo Spark** (~15h)
- 3 notebooks Jupyter interactivos
- Dataset e-commerce (tipo Instacart, 3.2M registros)
- TecnologÃ­as: PySpark, Pandas, visualizaciÃ³n

### **Objetivos de Aprendizaje**
- âœ… Configurar entornos Big Data desde cero
- âœ… Procesar datos distribuidos con Hadoop MapReduce
- âœ… Analizar datos con Hive (SQL sobre Hadoop)
- âœ… Procesar en memoria con Apache Spark
- âœ… Comparar Pandas vs Spark para diferentes volÃºmenes de datos

---

## ï¿½ Descargar Datasets

Los archivos de datos estÃ¡n hospedados en **Google Drive** debido a su tamaÃ±o (>650 MB total):

**ğŸ”— [Descargar Datasets E-commerce y MovieLens](https://drive.google.com/file/d/1AYd3PUi53Q7XynCg_7KKraUZYft3kjjo/view?usp=sharing)**

**Instrucciones:**
1. Descargar el archivo desde el enlace de arriba
2. Extraer el contenido en las carpetas correspondientes:
   - `Spark/data/` â†’ archivos CSV de e-commerce
   - `Hadoop/ml-100k/` â†’ archivos del dataset MovieLens
3. Ejecutar los notebooks y prÃ¡cticas

**Contenido del archivo:**
- `orders.csv` (103 MB) - Pedidos de clientes
- `order_products__prior.csv` (550 MB) - Productos en pedidos
- `products.csv`, `aisles.csv`, `departments.csv` - CatÃ¡logos
- `u.data`, `u.user`, `u.item` - Dataset MovieLens 100K

---

## ï¿½ğŸ“‚ Estructura del Workspace

```
Articulos/
â”œâ”€â”€ README.md                    # Esta guÃ­a
â”‚
â”œâ”€â”€ Hadoop/                      # MÃ³dulo Hadoop
â”‚   â”œâ”€â”€ Guia_CentOS_Hadoop_Hive.md              # InstalaciÃ³n completa (27 pasos)
â”‚   â”œâ”€â”€ Guia_Video_Instalacion_Hadoop.md        # GuÃ­a rÃ¡pida para video tutorial
â”‚   â”œâ”€â”€ Explicacion_Archivos_Configuracion_Hadoop.md  # ConfiguraciÃ³n XML
â”‚   â”œâ”€â”€ Comandos_Hadoop_Explicados.md           # Referencia de comandos
â”‚   â”œâ”€â”€ Practica_Hadoop_Hive_MovieLens.md       # PrÃ¡ctica avanzada 100K registros
â”‚   â”œâ”€â”€ Guia_Haddop_&_HIVE.md                   # PrÃ¡ctica bÃ¡sica de ventas
â”‚   â””â”€â”€ ml-100k/                                # Dataset MovieLens
â”‚       â”œâ”€â”€ u.data              # 100,000 ratings
â”‚       â”œâ”€â”€ u.user              # 943 usuarios
â”‚       â””â”€â”€ u.item              # 1,682 pelÃ­culas
â”‚
â””â”€â”€ Spark/                       # MÃ³dulo Spark
    â”œâ”€â”€ Big_Data.ipynb                          # Conceptos y caso de estudio
    â”œâ”€â”€ Analisis_Pandas.ipynb                   # AnÃ¡lisis con Pandas
    â”œâ”€â”€ Analisis_Spark.ipynb                    # AnÃ¡lisis con PySpark
    â””â”€â”€ data/                                   # Dataset e-commerce
        â”œâ”€â”€ orders.csv          # 206K pedidos
        â”œâ”€â”€ order_products__prior.csv  # 3.2M productos
        â”œâ”€â”€ products.csv        # 49K productos
        â”œâ”€â”€ aisles.csv          # 134 pasillos
        â””â”€â”€ departments.csv     # 21 departamentos
```

---

## ğŸ“š Contenido Detallado

### ğŸ“ **Carpeta Hadoop/**

Material para aprender el ecosistema Hadoop (HDFS, MapReduce, Hive):

| Archivo | DescripciÃ³n | Tiempo |
|---------|-------------|--------|
| [Guia_CentOS_Hadoop_Hive.md](Hadoop/Guia_CentOS_Hadoop_Hive.md) | InstalaciÃ³n completa paso a paso (27 pasos) | 6-8h |
| [Guia_Video_Instalacion_Hadoop.md](Hadoop/Guia_Video_Instalacion_Hadoop.md) | GuÃ­a condensada para video tutorial (20-30 min) con explicaciones detalladas de comandos bash, XMLs y archivos .env | 2-3h |
| [Explicacion_Archivos_Configuracion_Hadoop.md](Hadoop/Explicacion_Archivos_Configuracion_Hadoop.md) | ExplicaciÃ³n de XMLs: core-site, hdfs-site, mapred-site, yarn-site | 4-5h |
| [Comandos_Hadoop_Explicados.md](Hadoop/Comandos_Hadoop_Explicados.md) | Referencia de comandos HDFS, YARN, Hive | 3-4h |
| [Practica_Hadoop_Hive_MovieLens.md](Hadoop/Practica_Hadoop_Hive_MovieLens.md) | PrÃ¡ctica avanzada con 100K registros reales | 5-6h |
| [Guia_Haddop_&_HIVE.md](Hadoop/Guia_Haddop_&_HIVE.md) | PrÃ¡ctica bÃ¡sica: anÃ¡lisis de ventas | 3-4h |

**Dataset: ml-100k/**
- `u.data`: 100,000 ratings de pelÃ­culas (1-5 estrellas)
- `u.user`: 943 usuarios con datos demogrÃ¡ficos (edad, gÃ©nero, ocupaciÃ³n)
- `u.item`: 1,682 pelÃ­culas con gÃ©neros

**QuÃ© aprenderÃ¡s:**
- Instalar y configurar Hadoop 3.4.0 en CentOS desde cero
- Comprender archivos de configuraciÃ³n XML (core-site, hdfs-site, mapred-site, yarn-site)
- Configurar variables de entorno (.bashrc, hadoop-env.sh, yarn-env.sh, hive-env.sh)
- Usar comandos HDFS para gestionar archivos distribuidos
- Crear tablas Hive y ejecutar consultas SQL sobre Big Data
- Analizar 100K registros reales con MapReduce
- Monitorear jobs en YARN Web UI
- Solucionar problemas comunes de configuraciÃ³n
- Grabar video tutoriales de instalaciÃ³n (guÃ­a para video)

---

### ğŸ“ **Carpeta Spark/**

Material para aprender Apache Spark y compararlo con Pandas:

| Notebook | DescripciÃ³n | Tiempo |
|----------|-------------|--------|
| [Big_Data.ipynb](Spark/Big_Data.ipynb) | Conceptos de Big Data y caso de estudio | 2-3h |
| [Analisis_Pandas.ipynb](Spark/Analisis_Pandas.ipynb) | AnÃ¡lisis exploratorio con Pandas (tradicional) | 4-5h |
| [Analisis_Spark.ipynb](Spark/Analisis_Spark.ipynb) | Mismo anÃ¡lisis con PySpark (distribuido) | 5-6h |

**Dataset: data/** (E-commerce tipo Instacart)
- `orders.csv`: 206,000 pedidos de clientes
- `order_products__prior.csv`: 3.2 millones de productos comprados
- `products.csv`: 49,000 productos del catÃ¡logo
- `aisles.csv` y `departments.csv`: CategorÃ­as

**QuÃ© aprenderÃ¡s:**
- Cargar y procesar CSVs con Pandas y PySpark
- Realizar anÃ¡lisis exploratorio de datos (EDA)
- Aplicar transformaciones lazy y acciones en Spark
- Usar cache y optimizaciones (broadcast, repartition)
- Crear visualizaciones con Matplotlib
- Market Basket Analysis (productos comprados juntos)
- Comparar rendimiento: Pandas vs Spark

**Contenido de los Notebooks:**

**1. Big_Data.ipynb**
- Contexto del problema (empresa e-commerce)
- MetodologÃ­a de anÃ¡lisis de Big Data
- Desarrollo prÃ¡ctico con Pandas (6 visualizaciones)
- Conceptos: volumen, variedad, velocidad

**2. Analisis_Pandas.ipynb**
- Preprocesamiento: carga, limpieza, merge
- AnÃ¡lisis de distribuciÃ³n de pedidos
- Patrones temporales (dÃ­a/hora)
- Top 5 productos mÃ¡s vendidos
- Productos por departamento
- Market Basket Analysis con NetworkX (grafo)

**3. Analisis_Spark.ipynb**
- ConfiguraciÃ³n de SparkSession
- Conceptos clave: RDD, Transformations, Actions, DAG
- Mismo anÃ¡lisis que Pandas pero con PySpark
- Window Functions (tÃ©cnica avanzada)
- UDFs (User Defined Functions)
- Optimizaciones: cache, broadcast, persist
- ComparaciÃ³n de rendimiento

---

## ğŸ›  TecnologÃ­as

### Hadoop Stack
- **CentOS** 9 - Sistema operativo Linux
- **Java** 8 - Runtime para Hadoop
- **Hadoop** 3.4.0 - Framework distribuido
- **HDFS** - Almacenamiento distribuido
- **YARN** - GestiÃ³n de recursos
- **MapReduce** - Procesamiento paralelo
- **Hive** 3.1.3 - SQL sobre Hadoop

### Spark Stack
- **Python** 3.8+ - Lenguaje principal
- **Apache Spark** 3.5 - Motor en memoria
- **PySpark** - API Python para Spark
- **Pandas** - AnÃ¡lisis de datos
- **Matplotlib/Seaborn** - VisualizaciÃ³n
- **NetworkX** - Grafos
- **Jupyter** - Notebooks interactivos

---

## ğŸ¯ Ruta de Aprendizaje

### **Semanas 1-3: MÃ³dulo Hadoop**

**Semana 1: InstalaciÃ³n**
- Instalar CentOS en VirtualBox
- Configurar usuario hadoop y Java
- Instalar Hadoop 3.4.0
- Configurar archivos XML

**Semana 2: Comandos y Hive**
- Practicar comandos HDFS
- Instalar Hive
- Crear tablas y consultas SQL

**Semana 3: PrÃ¡cticas**
- AnÃ¡lisis de ventas (bÃ¡sico)
- Dataset MovieLens 100K (avanzado)

### **Semanas 4-5: MÃ³dulo Spark**

**Semana 4: Pandas**
- Instalar Python y librerÃ­as
- Notebook Big_Data.ipynb
- Notebook Analisis_Pandas.ipynb

**Semana 5: PySpark**
- Notebook Analisis_Spark.ipynb
- Comparar Pandas vs Spark
- Optimizaciones avanzadas

---

## ğŸ“Š ComparaciÃ³n: Hadoop vs Spark

| CaracterÃ­stica | Hadoop MapReduce | Apache Spark |
|----------------|------------------|--------------|
| **Velocidad** | Disco (lento) | Memoria (100x mÃ¡s rÃ¡pido) |
| **Facilidad** | HiveQL (SQL) | Python + SQL |
| **Casos de uso** | ETL masivos, batch | Analytics, ML, streaming |
| **CuÃ¡ndo usar** | TB-PB datos histÃ³ricos | AnÃ¡lisis iterativo, interactivo |

---

## âš ï¸ SoluciÃ³n de Problemas

### **Hadoop**

**Error: "Permission denied: user=dr.who"**
```xml
<!-- Agregar a core-site.xml -->
<property>
    <name>hadoop.http.staticuser.user</name>
    <value>hadoop</value>
</property>
```

**Error: "Return Code 2" en Hive**
```sql
SET hive.execution.engine=mr;
SET mapreduce.framework.name=yarn;
```

**HDFS en Safe Mode**
```bash
hdfs dfsadmin -safemode leave
```

### **Spark**

**OutOfMemoryError**
```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "8g") \
    .getOrCreate()
```

**Lentitud en joins**
```python
from pyspark.sql.functions import broadcast
result = large_df.join(broadcast(small_df), "key")
```

---

## ğŸ“ Comandos Ãštiles

### **HDFS**
```bash
hdfs dfs -ls /                    # Listar
hdfs dfs -put local.txt /hdfs/    # Subir
hdfs dfs -get /hdfs/file.txt .    # Descargar
hdfs dfs -cat /archivo.txt        # Ver contenido
```

### **Hive**
```sql
SHOW DATABASES;
CREATE DATABASE nombre;
USE nombre;
SHOW TABLES;
SELECT * FROM tabla LIMIT 10;
```

### **Spark**
```python
# Leer CSV
df = spark.read.csv("file.csv", header=True, inferSchema=True)

# Transformaciones
df.filter(col("age") > 18).select("name", "age")

# Acciones
df.count()
df.show()
```

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n Oficial
- **Hadoop:** https://hadoop.apache.org/docs/stable/
- **Hive:** https://cwiki.apache.org/confluence/display/Hive/
- **Spark:** https://spark.apache.org/docs/latest/
- **PySpark:** https://spark.apache.org/docs/latest/api/python/

### Interfaces Web (Hadoop)
- HDFS NameNode: http://localhost:9870
- YARN ResourceManager: http://localhost:8088
- MapReduce JobHistory: http://localhost:19888

---

<div align="center">

**Â¡Ã‰xito en tu aprendizaje de Big Data!**

*Universidad ISU - Febrero 2026*

</div>
