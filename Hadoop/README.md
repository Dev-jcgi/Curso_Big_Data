# üêò M√≥dulo Hadoop

> Aprende a instalar, configurar y usar el ecosistema Hadoop (HDFS, YARN, MapReduce, Hive)

---

## üìã Gu√≠as de Estudio

### 1Ô∏è‚É£ [Instalacion_Configuracion_Hadop_Hive.md](Instalacion_Configuracion_Hadop_Hive.md)
**Instalaci√≥n completa paso a paso**

- Configurar CentOS 9 y usuario hadoop
- Instalar Java 8, Hadoop 3.4.0 y Hive 3.1.3
- Configurar archivos XML (usar carpeta `Archivos_Configuracion/`)
- Iniciar servicios: `start-all.sh`
- Verificar: http://localhost:9870 y http://localhost:8088

### 2Ô∏è‚É£ [Comandos_Hadoop_Explicados.md](Comandos_Hadoop_Explicados.md)
**Referencia de comandos HDFS, YARN y Hive**

- Comandos b√°sicos HDFS: `ls`, `put`, `get`, `cat`, `rm`
- Gesti√≥n YARN: monitoreo de jobs y recursos
- Comandos Hive: crear tablas, cargar datos, consultas SQL

### 3Ô∏è‚É£ [Explicacion_Archivos_Configuracion_Hadoop.md](Explicacion_Archivos_Configuracion_Hadoop.md)
**Detalles de los archivos XML**

- `core-site.xml`: NameNode y propiedades core
- `hdfs-site.xml`: Replicaci√≥n y directorios HDFS
- `mapred-site.xml`: MapReduce sobre YARN
- `yarn-site.xml`: ResourceManager y NodeManager
- `hive-site.xml`: Metastore y warehouse de Hive

---

## üéØ Pr√°cticas

### üìä [Practica_Basica_Hive.md](Practica_Basica_Hive.md)
**An√°lisis b√°sico con Hive**

**Objetivo:** Aprender a crear tablas, cargar datos y ejecutar consultas SQL en Hive

**Dataset:** Datos de ventas (productos, categor√≠as, ventas)

**Ejercicios:**
- Crear base de datos y tablas en Hive
- Cargar datos desde archivos locales y HDFS
- Consultas de agregaci√≥n (COUNT, SUM, AVG, GROUP BY)
- Joins entre tablas
- Exportar resultados

---

### üé¨ [Practica_Hive_MovieLens.md](Practica_Hive_MovieLens.md)
**Pr√°ctica avanzada con 100K registros reales**

**Objetivo:** Analizar el dataset MovieLens con t√©cnicas avanzadas de Hive

**Dataset:** `dataset/ml-100k/`
- `u.data`: 100,000 ratings (userid, movieid, rating, timestamp)
- `u.user`: 943 usuarios (edad, g√©nero, ocupaci√≥n)
- `u.item`: 1,682 pel√≠culas (t√≠tulo, fecha, g√©neros)

**Ejercicios:**
1. **PASO 0:** Iniciar servicios Hadoop/Hive
2. **PASO 1:** Crear base de datos y cargar datos
3. **PASO 2:** Consultas b√°sicas (top pel√≠culas, usuarios activos)
4. **PASO 3:** An√°lisis demogr√°fico (patrones por edad/g√©nero)
5. **PASO 4:** Joins complejos (combinar ratings + usuarios + pel√≠culas)
6. **PASO 5:** An√°lisis temporal (tendencias por a√±o/mes)
7. **PASO 6:** Consultas avanzadas (subconsultas, window functions)

**T√©cnicas avanzadas:**
- Tablas particionadas
- Window functions (RANK, ROW_NUMBER)
- Subconsultas correlacionadas
- Optimizaci√≥n de queries

---

## üìÅ Archivos de Configuraci√≥n

La carpeta `Archivos_Configuracion/` contiene XMLs listos para usar:

```
Archivos_Configuracion/
‚îú‚îÄ‚îÄ core-site.xml      # NameNode: hdfs://localhost:9000
‚îú‚îÄ‚îÄ hdfs-site.xml      # Replicaci√≥n, directorios
‚îú‚îÄ‚îÄ mapred-site.xml    # Framework: YARN
‚îú‚îÄ‚îÄ yarn-site.xml      # ResourceManager, NodeManager
‚îú‚îÄ‚îÄ hive-site.xml      # Metastore Derby, warehouse
‚îî‚îÄ‚îÄ hive-env.sh        # JAVA_HOME, HADOOP_HOME
```

**Uso:** Copiar los archivos a `$HADOOP_HOME/etc/hadoop/` y `$HIVE_HOME/conf/`

---

## üìä Dataset MovieLens

Ubicaci√≥n: `dataset/ml-100k/`

| Archivo | Registros | Descripci√≥n |
|---------|-----------|-------------|
| u.data | 100,000 | Ratings de pel√≠culas (1-5 estrellas) |
| u.user | 943 | Usuarios (edad, g√©nero, ocupaci√≥n) |
| u.item | 1,682 | Pel√≠culas (t√≠tulo, fecha, g√©neros) |
| u.genre | 19 | Lista de g√©neros cinematogr√°ficos |

---

## üöÄ Inicio R√°pido

```bash
# 1. Iniciar servicios
start-all.sh

# 2. Verificar HDFS y YARN
jps

# 3. Abrir Hive
hive

# 4. Crear base de datos
CREATE DATABASE movielens;
USE movielens;

# 5. Seguir las pr√°cticas
```

---

## üåê Interfaces Web

- **HDFS NameNode:** http://localhost:9870
- **YARN ResourceManager:** http://localhost:8088
- **MapReduce JobHistory:** http://localhost:19888

---

## üí° Tips

- Siempre verificar que los servicios est√©n corriendo con `jps`
- Usar `hdfs dfs -ls /` para verificar HDFS
- Los logs est√°n en `$HADOOP_HOME/logs/`
- Detener servicios: `stop-all.sh`

---

**¬°√âxito con Hadoop! üéâ**
