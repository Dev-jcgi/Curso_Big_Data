# ğŸš€ GuÃ­a Definitiva: Hadoop 3.4.0 + Hive 3.3.1

> **AnÃ¡lisis de Ventas con SoluciÃ³n Completa de Errores Comunes**

**Autor:** GuÃ­a PrÃ¡ctica Hadoop
**Fecha:** Febrero 2026
**Versiones:** Hadoop 3.4.0 | Hive 3.3.1 | Java 8

---

## ğŸ› ï¸ PASO 0: ConfiguraciÃ³n CrÃ­tica (Evitar Errores)

> âš ï¸ **IMPORTANTE:** Esta secciÃ³n resuelve los 3 problemas mÃ¡s comunes que bloquean Hive

### Problemas que resolveremos:

| # | Error                            | SoluciÃ³n               |
| - | -------------------------------- | ----------------------- |
| 1 | Java 17 incompatible             | Forzar Java 8           |
| 2 | "Permission denied: user=dr.who" | Configurar usuario web  |
| 3 | "Return Code 2"                  | Deshabilitar modo local |

---

### A. Forzar Java 8 y Hadoop Home

#### ğŸ”§ ConfiguraciÃ³n de Variables de Entorno

**Archivos a editar:**

- `/opt/hadoop/hadoop-3.4.0/etc/hadoop/hadoop-env.sh`
- `/opt/hive/conf/hive-env.sh`

```bash
# 1. Crear hive-env.sh desde la plantilla
cp /opt/hive/conf/hive-env.sh.template /opt/hive/conf/hive-env.sh

# 2. Editar AMBOS archivos de entorno
# Agregar o verificar estas lÃ­neas:
export JAVA_HOME=/opt/jdk1.8.0_202
export HADOOP_HOME=/opt/hadoop/hadoop-3.4.0

# 3. Reiniciar Hive
Si esta en ejecucion salir de la sesion con:
exit;
```

**âœ… Verificar:**

```bash
echo $JAVA_HOME
echo $HADOOP_HOME
java -version  # Debe mostrar 1.8.0_202
```

---

### B. Corregir el Error "Permission denied: user=dr.who"

#### ğŸ”§ Configurar Usuario Web de Hadoop

**Archivo:** `/opt/hadoop/hadoop-3.4.0/etc/hadoop/core-site.xml`

**Agregar dentro de `<configuration>`:**

```xml
<property>
    <name>hadoop.http.staticuser.user</name>
    <value>hadoop</value>
    <description>Usuario para interfaz web HDFS</description>
</property>
```

**ğŸ’¡ ExplicaciÃ³n:** Esto evita que la interfaz web de HDFS use el usuario ficticio "dr.who".

---

### C. Limpiar Permisos de Carpetas Temporales

#### ğŸ”§ Comandos de Permisos

```bash
# Dar permisos completos al filesystem HDFS
hdfs dfs -chmod -R 777 /

# Crear y configurar directorio staging de YARN
hdfs dfs -mkdir -p /tmp/hadoop-yarn/staging
hdfs dfs -chmod -R 777 /tmp/hadoop-yarn/staging
```

**âœ… Verificar:**

```bash
hdfs dfs -ls /
hdfs dfs -ls /tmp/hadoop-yarn/
```

---

## ğŸ¯ PASO 1: Iniciar Servicios

â±ï¸ **Tiempo estimado:** 2 minutos

### 1.1 Cargar Variables de Entorno

```bash
export HADOOP_HOME=/opt/hadoop/hadoop-3.4.0
export HIVE_HOME=/opt/hive
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$PATH
```

### 1.2 Reiniciar Servicios Hadoop

```bash
# Detener todos los servicios
stop-all.sh

# Iniciar HDFS
start-dfs.sh

# Iniciar YARN
start-yarn.sh

# Verificar servicios activos
jps
```

**Salida esperada de `jps`:**

```
12345 NameNode
12346 DataNode
12347 ResourceManager
12348 NodeManager
12349 SecondaryNameNode
```

### 1.3 Conectar a Hive

```bash
# Conectar a Hive directamente
hive
```

**âœ… Verificar conexiÃ³n:**

```bash
# Dentro de Hive, ejecutar:
SHOW DATABASES;
```

---

## ğŸ“Š PASO 2: Cargar Datos en HDFS

â±ï¸ **Tiempo estimado:** 1 minuto

### 2.1 Crear Directorio en HDFS

```bash
hdfs dfs -mkdir -p /demo/ventas
```

### 2.2 Cargar Dataset Directamente

> **Nota:** Los datos se cargan desde STDIN sin archivo intermedio

```bash
hdfs dfs -put - /demo/ventas/datos.txt << 'EOF'
1001	Laptop	Dell	1200	2024-01-15	Norte
1002	Mouse	Logitech	25	2024-01-15	Sur
1003	Teclado	HP	45	2024-01-16	Este
1004	Monitor	Samsung	350	2024-01-16	Norte
1005	Laptop	HP	1100	2024-01-17	Oeste
1006	Mouse	Dell	30	2024-01-17	Norte
1007	Teclado	Logitech	50	2024-01-18	Sur
1008	Monitor	LG	400	2024-01-18	Este
1009	Laptop	Lenovo	1300	2024-01-19	Norte
1010	Mouse	HP	20	2024-01-19	Sur
1011	Teclado	Dell	40	2024-01-20	Oeste
1012	Monitor	Dell	380	2024-01-20	Norte
1013	Laptop	Asus	1250	2024-01-21	Este
1014	Mouse	Lenovo	28	2024-01-21	Norte
1015	Teclado	Samsung	42	2024-01-22	Sur
EOF
```

### 2.3 Verificar Datos Cargados

```bash
# Ver contenido del archivo
hdfs dfs -cat /demo/ventas/datos.txt

# Contar lÃ­neas
hdfs dfs -cat /demo/ventas/datos.txt | wc -l

# Ver primeras 5 lÃ­neas
hdfs dfs -cat /demo/ventas/datos.txt | head -5
```

### ğŸ“‹ Estructura del Dataset

| Campo        | Tipo   | DescripciÃ³n            | Ejemplo    |
| ------------ | ------ | ----------------------- | ---------- |
| `id`       | INT    | Identificador Ãºnico    | 1001       |
| `producto` | STRING | CategorÃ­a del producto | Laptop     |
| `marca`    | STRING | Fabricante              | Dell       |
| `precio`   | DOUBLE | Precio en USD           | 1200       |
| `fecha`    | STRING | Fecha de venta          | 2024-01-15 |
| `region`   | STRING | Zona geogrÃ¡fica        | Norte      |

**Total de registros:** 15

---

## ğŸ PASO 3: EjecuciÃ³n de Consultas (Hive)

â±ï¸ **Tiempo estimado:** 5 minutos

---

### 3.1 âš ï¸ CONFIGURACIÃ“N OBLIGATORIA (Evitar "Return Code 2")

> **CRÃTICO:** Ejecutar estos comandos ANTES de cualquier consulta

```sql
-- Deshabilitar modo de ejecuciÃ³n local
SET hive.exec.mode.local.auto=false;

-- Forzar uso de YARN para MapReduce
SET mapreduce.framework.name=yarn;
```

**ğŸ’¡ ExplicaciÃ³n:**

- Sin estos comandos, Hive intenta ejecutar queries en memoria local
- Esto causa "Return Code 2" en consultas con GROUP BY
- Forzamos que YARN distribuya el trabajo correctamente

---

### 3.2 Crear Base de Datos y Tabla

#### Paso 1: Crear Base de Datos

```sql
CREATE DATABASE IF NOT EXISTS tienda;
USE tienda;
```

#### Paso 2: Crear Tabla Externa

```sql
CREATE EXTERNAL TABLE ventas (
    id INT,
    producto STRING,
    marca STRING,
    precio DOUBLE,
    fecha STRING,
    region STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/demo/ventas/';
```

#### Paso 3: Verificar Tabla

```sql
-- Ver estructura
DESCRIBE ventas;

-- Ver datos
SELECT * FROM ventas LIMIT 5;

-- Contar registros
SELECT COUNT(*) as total FROM ventas;
```

---

### 3.3 Consulta Principal: AgregaciÃ³n de Ventas

#### ğŸ“ SQL:

```sql
SELECT 
    producto,
    COUNT(*) as cantidad_vendida,
    ROUND(SUM(precio), 2) as total_ingresos,
    ROUND(AVG(precio), 2) as precio_promedio
FROM ventas
GROUP BY producto
ORDER BY total_ingresos DESC;
```

#### ğŸ“Š Resultado Esperado:

| producto | cantidad_vendida | total_ingresos | precio_promedio |
| -------- | ---------------- | -------------- | --------------- |
| Laptop   | 4                | 4850.00        | 1212.50         |
| Monitor  | 3                | 1130.00        | 376.67          |
| Teclado  | 4                | 177.00         | 44.25           |
| Mouse    | 4                | 103.00         | 25.75           |

#### ğŸ¯ Flujo MapReduce:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT (HDFS)â”‚  15 registros en /demo/ventas/datos.txt
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MAPPER PHASE    â”‚  Lee y emite <producto, precio>
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SHUFFLE & SORT   â”‚  Agrupa por producto
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REDUCER PHASE    â”‚  Calcula COUNT, SUM, AVG
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚OUTPUT (HDFS)â”‚  Resultados agregados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” PASO 4: Monitoreo y Resultados

â±ï¸ **Tiempo:** Continuo durante ejecuciÃ³n

### 4.1 Interfaces Web de Monitoreo

| Servicio                        | URL                                           | InformaciÃ³n Visible                       |
| ------------------------------- | --------------------------------------------- | ------------------------------------------ |
| **YARN Resource Manager** | [http://localhost:8088](http://localhost:8088)   | Jobs MapReduce, Mappers, Reducers, Memoria |
| **HDFS NameNode**         | [http://localhost:9870](http://localhost:9870)   | Explorador de archivos, Bloques, Nodos     |
| **Job History Server**    | [http://localhost:19888](http://localhost:19888) | Historial de jobs completados              |

### 4.2 Monitoreo desde Terminal

#### Ver aplicaciones YARN activas:

```bash
# Listar aplicaciones en ejecuciÃ³n
watch -n 2 'yarn application -list'

# Ver estado de una aplicaciÃ³n especÃ­fica
yarn application -status application_1234567890_0001
```

#### Ver logs en tiempo real:

```bash
# Logs de Hive
tail -f /tmp/hive.log

# Logs de aplicaciÃ³n YARN
yarn logs -applicationId application_1234567890_0001
```

### 4.3 MÃ©tricas a Observar

âœ… **En YARN Web UI:**

- NÃºmero de mappers ejecutados
- NÃºmero de reducers asignados
- Memoria consumida
- Tiempo de cada fase (Map/Shuffle/Reduce)
- Estado: RUNNING â†’ SUCCEEDED

âœ… **En HDFS Web UI:**

- Bloques de datos
- ReplicaciÃ³n
- Nodos DataNode activos
- Archivos generados

---

## ğŸ“¤ PASO 5: Exportar Resultados

â±ï¸ **Tiempo estimado:** 1 minuto

### 5.1 Exportar a Directorio Local (CSV)

#### En Hive:

```sql
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/resumen_ventas'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT 
    producto,
    COUNT(*) as cantidad,
    ROUND(SUM(precio), 2) as total
FROM ventas
GROUP BY producto
ORDER BY total DESC;
```

#### En Terminal:

```bash
# Ver resultados
cat /tmp/resumen_ventas/000000_0

# O con formato:
column -t -s',' /tmp/resumen_ventas/000000_0
```

### 5.2 Exportar a HDFS (para compartir)

#### En Hive:

```sql
INSERT OVERWRITE DIRECTORY '/demo/resultados/por_producto'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT 
    producto,
    COUNT(*) as cantidad,
    ROUND(SUM(precio), 2) as total
FROM ventas
GROUP BY producto
ORDER BY total DESC;
```

#### En Terminal:

```bash
# Ver resultado en HDFS
hdfs dfs -cat /demo/resultados/por_producto/*

# Descargar localmente (especificar ruta absoluta)
hdfs dfs -get /demo/resultados/por_producto/000000_0 /home/hadoop/resumen.csv

# Ver contenido del archivo descargado
cat /home/hadoop/resumen.csv
```

---

## ğŸ§¹ LIMPIEZA (Opcional)

â±ï¸ **Tiempo estimado:** 2 minutos

---

### Detener Servicios

```bash
# Salir de Hive (si estÃ¡s dentro)
quit;

# Detener YARN y HDFS
stop-yarn.sh
stop-dfs.sh

# O detener todo a la vez
stop-all.sh
```

---

### Limpiar Datos

#### En Hive:

```sql
-- Eliminar tabla
DROP TABLE IF EXISTS ventas;

-- Eliminar base de datos
DROP DATABASE IF EXISTS tienda;

-- Salir
quit;
```

#### En Terminal:

```bash
# Eliminar datos en HDFS
hdfs dfs -rm -r /demo/

# Eliminar resultados locales exportados
hdfs dfs -rm -r /tmp/resumen_ventas/

# Limpiar archivos locales
rm -rf /tmp/resumen_ventas/
rm -f /home/hadoop/resumen.csv
```

---

## ğŸ“– CONSULTAS ADICIONALES (Bonus)

### âœ… Consulta 2: Ventas por regiÃ³n

```sql
SELECT 
    region,
    COUNT(*) as num_ventas,
    ROUND(SUM(precio), 2) as ingresos_totales,
    ROUND(AVG(precio), 2) as ticket_promedio
FROM ventas
GROUP BY region
ORDER BY ingresos_totales DESC;
```

**Resultado esperado:**

```
Norte    7    3653.00    521.86
Este     3    1795.00    598.33
Sur      3     117.00     39.00
Oeste    2    1185.00    592.50
```

---

### âœ… Consulta 3: Marca mÃ¡s vendida por producto

```sql
SELECT 
    producto,
    marca,
    COUNT(*) as ventas,
    ROUND(SUM(precio), 2) as ingresos
FROM ventas
GROUP BY producto, marca
ORDER BY producto, ventas DESC;
```

---

### âœ… Consulta 4: AnÃ¡lisis temporal (por fecha)

```sql
SELECT 
    fecha,
    COUNT(*) as ventas_dia,
    ROUND(SUM(precio), 2) as ingresos_dia,
    COUNT(DISTINCT producto) as productos_diferentes
FROM ventas
GROUP BY fecha
ORDER BY fecha;
```

---

### âœ… Consulta 5: Top productos mÃ¡s caros

```sql
SELECT 
    producto,
    marca,
    precio,
    region,
    fecha
FROM ventas
WHERE precio > 100
ORDER BY precio DESC
LIMIT 10;
```

---

### âœ… Consulta 6: EstadÃ­sticas avanzadas

```sql
SELECT 
    producto,
    COUNT(*) as cantidad,
    MIN(precio) as precio_min,
    MAX(precio) as precio_max,
    ROUND(AVG(precio), 2) as precio_prom,
    ROUND(STDDEV(precio), 2) as desviacion,
    PERCENTILE_APPROX(precio, 0.5) as mediana
FROM ventas
GROUP BY producto
ORDER BY cantidad DESC;
```

**ğŸ¯ Concepto:** Las funciones estadÃ­sticas (AVG, STDDEV, PERCENTILE) se calculan **distribuidas** entre mÃºltiples reducers.

---

### âœ… Consulta 7: Crear tabla de resumen (para reusar)

```sql
-- Crear tabla con resultados agregados
CREATE TABLE resumen_ventas AS
SELECT 
    producto,
    COUNT(*) as cantidad,
    ROUND(SUM(precio), 2) as ingresos,
    ROUND(AVG(precio), 2) as precio_prom
FROM ventas
GROUP BY producto;

-- Ver resultados
SELECT * FROM resumen_ventas;

-- Esta tabla ahora estÃ¡ en HDFS y se puede consultar rÃ¡pidamente
DESCRIBE FORMATTED resumen_ventas;
```

---

### âœ… Consulta 8: Filtrado con HAVING

```sql
-- Solo productos con mÃ¡s de 2 ventas
SELECT 
    producto,
    marca,
    COUNT(*) as ventas,
    ROUND(AVG(precio), 2) as precio_prom
FROM ventas
GROUP BY producto, marca
HAVING COUNT(*) >= 2
ORDER BY ventas DESC;
```

---

## ğŸ“š RESUMEN DE SOLUCIONES

### âœ… Errores Resueltos

| # | Error Original                   | SoluciÃ³n Aplicada                                                  |
| - | -------------------------------- | ------------------------------------------------------------------- |
| 1 | Java 17 causa fallos             | Forzar Java 8 en `hadoop-env.sh` y `hive-env.sh`                |
| 2 | "Permission denied: user=dr.who" | Agregar `hadoop.http.staticuser.user=hadoop` en `core-site.xml` |
| 3 | "Return Code 2" en queries       | `SET hive.exec.mode.local.auto=false` antes de consultas          |
| 4 | Permisos en staging              | `hdfs dfs -chmod -R 777 /tmp/hadoop-yarn/staging`                 |

### ğŸ¯ Comandos Clave

#### Inicio RÃ¡pido:

```bash
# 1. Iniciar servicios Hadoop
start-dfs.sh && start-yarn.sh

# 2. Verificar servicios
jps
```

#### Conectar a Hive:

```bash
# Conectar directamente a Hive
hive
```

#### ConfiguraciÃ³n Obligatoria en Hive:

```sql
SET hive.exec.mode.local.auto=false;
SET mapreduce.framework.name=yarn;
```

#### Monitoreo:

```bash
# Ver aplicaciones YARN
watch -n 2 'yarn application -list'

# Ver jobs en tiempo real
http://localhost:8088
```

### ğŸ“Š Resultado Final

âœ… Dataset de 15 ventas cargado en HDFS
âœ… Tabla Hive externa creada
âœ… Consulta principal con GROUP BY ejecutada vÃ­a MapReduce
âœ… 7 consultas adicionales con anÃ¡lisis avanzados
âœ… Resultados exportados a CSV local
âœ… Todos los errores comunes resueltos

---

## ğŸ”— RECURSOS ADICIONALES

### DocumentaciÃ³n Oficial:

| Recurso                            | URL                                                                                         |
| ---------------------------------- | ------------------------------------------------------------------------------------------- |
| **Hadoop 3.4.0 Docs**        | https://hadoop.apache.org/docs/r3.4.0/                                                      |
| **Hive Language Manual**     | https://cwiki.apache.org/confluence/display/Hive/LanguageManual                             |
| **YARN ResourceManager API** | https://hadoop.apache.org/docs/r3.4.0/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html |
| **HDFS Commands Guide**      | https://hadoop.apache.org/docs/r3.4.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html     |

### Herramientas de Monitoreo:

- ğŸŒ **YARN UI:** http://localhost:8088
- ğŸŒ **HDFS UI:** http://localhost:9870
- ğŸŒ **Job History:** http://localhost:19888

---

<div align="center">

## ğŸ‰ Â¡PrÃ¡ctica Completada!

**Hadoop 3.4.0 + Hive 3.3.1 funcionando correctamente**

âœ… MapReduce distribuido
âœ… SQL sobre Big Data
âœ… Todos los errores resueltos

*GuÃ­a creada en Febrero 2026*

</div>
